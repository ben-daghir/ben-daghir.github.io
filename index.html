<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Machine Learning for Satellite Crop Classification</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/normalize.css" media="screen">
    <link rel="stylesheet" type="text/css" href="css/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="css/github-light.css" media="screen">
    <link rel="stylesheet" href="css/style.css">
    <script src="js/jquery.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="js/jquery.csv.min.js"></script>
    <script type="text/javascript" src="js/latex.js"></script>
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Machine Learning for Satellite Crop Classification</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.gatech.edu/kvu35/CS4641-Project" class="btn">View on GitHub</a>
    </section>

    <section class="main-content">
      <h2>The Problem and Motivation
        <a id="Introduction" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span>
        </a>
      </h2>
      <p>
        Agriculture is very important to the global community, not only to provide a basic need for human survival but
        also as a main driver of economic growth, especially in developing countries. Having accurate and reliable
        agricultural data is essential for food security and to monitor economic growth. Having data taken from
        satellites of different regions over time can be helpful in building models to monitor agriculture to help
        increase farming productivty and prevent possible losses. Compared to surveying land using traditional methods,
        being able to use satellite data provides more accuracy and provides data at a scale with high spatial granularity.
        However, the large amount of data taken needs to be able to be analyzed efficiently to help predict useful
        information. For this project we chose to focus on the problem of identifying crop types from satellite images
        as there are many different types of crops being grown in a region and it is essential to first identify the
        crop type before analyzing any trends.
      </p>
    </section>

    <section>
      <div class="main-content">

        <h2>
          The Data
          <a id="Data" class="anchor" href="#designer-templates" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span></a>
        </h2>
        <p>
          The dataset we used for our project comes from the <a target="_blank" href="http://docs.mlhub.earth/#radiant-mlhub-api>">
          Radiant Earth MLHub</a> and was accessed via a RESTful API.
          The dataset contains over four thousand time-series raster images of farmland fields taken by a Sentinel-2
          satellite. The images taken consist of 12 separate frequency spectrum imaging bands and cloud interference
          probability for each pixel. Images are taken every month within the growing cycle and multiple fields can be
          present in one image which results in some fields only being a few pixels wide. We used 3286 images for training
          our models and 1402 images were provided for testing. Our project will attempt to correctly classify the results
          into the 7 Crop IDs provided by the dataset with the highest accuracy possible.
        </p>
      </div>

      <div id="image-visualizer" class="container">
        <div id="image-k" class="image">
            <img src="images/k.png" class="image image-k"/>
            <span style="opacity: 0" class="image-caption">Bands 01, 05 - 09, CLD <br>Invisible Spectrum, Cloud Data</span>
            <span style="opacity: 0; top:-65px;" class="diagram-caption">All Image Spectral Bands<br>(Features)</span>
        </div>
        <div id="image-b" class="image">
            <img src="images/b.png" class="image image-b"/>
            <span style="opacity: 0" class="image-caption">Band 02<br>Visible Blue</span>
        </div>
        <div id="image-g" class="image">
            <img src="images/g.png" class="image image-g"/>
            <span style="opacity: 0" class="image-caption">Band 03<br>Visible Green</span>
        </div>
        <div id="image-r" class="image">
            <img src="images/r.png" class="image image-r"/>
            <span style="opacity: 0" class="image-caption">Band 04<br>Visible Red</span>
        </div>
        <div id="image-fids" class="image image-fids" style="opacity:0">
            <img src="images/fids.png" class="image"/>
            <span style="opacity: 0" class="diagram-caption">Field IDs<br>(Pre-Processing Data)</span>
        </div>
        <div id="image-labels" class="image image-labels" style="opacity:0">
            <img src="images/labels.png" class="image"/>
            <span style="opacity: 0" class="diagram-caption">Crop IDs<br>(Labels)</span>
        </div>
        <div id="floating-label" class="image floating-label">
          Scroll Over Me!
        </div>
        <div id="scroll-container" class="container">
            <div id="scroll-anchor" class="anchor"></div>
        </div>
      </div>
      <div class="main-content">
        <p>
          Below is a table indicating the Crop IDs and their corresponding Crop Type.
        </p>
        <table class="w3-hoverable" style="width:max-content; text-align:center; margin:auto;">
          <tr class="w3-green">
            <th>Crop ID</th>
            <th>Crop Type</th>
            <th>Number of Fields</th>
          </tr>
          <tr>
            <td>0</td>
            <td>Maize</td>
            <td>1462</td>
          </tr>
          <tr>
            <td>1</td>
            <td>Cassava</td>
            <td>829</td>
          </tr>
          <tr>
            <td>2</td>
            <td>Common Bean</td>
            <td>98</td>
          </tr>
          <tr>
            <td>3</td>
            <td>Maize & Common Bean (intercropping)</td>
            <td>487</td>
          </tr>
          <tr>
            <td>4</td>
            <td>Maize & Cassava (intercropping)</td>
            <td>172</td>
          </tr>
          <tr>
            <td>5</td>
            <td>Maize & Soybean (intercropping)</td>
            <td>160</td>
          </tr>
          <tr>
            <td>6</td>
            <td>Cassava & Common Bean (intercropping)</td>
            <td>78</td>
          </tr>
        </table>
      </div>

      <div class="main-content">
        <p>
          The tool below, known as a Parallel Coordinate plot or "ParaCoord" plot, is extremely useful diplaying relationships between
          the each field's features (each spectral band reading) and respective label (Crop ID). Below is a quick tutorial
          on how to effectively use the tool.<br><br>
          <div class="wrap-collabsible">
            <input id="collapsible" class="toggle" type="checkbox">
            <label for="collapsible" class="lbl-toggle">Tutorial</label>
            <div class="collapsible-content">
              <div class="content-inner">
                Each continuous color-coded line spanning left-to-right corresponds to a single data point. There are multiple y-axes,
                13 to be exact, labeled at the top as either a spectral band (feature) or the data point's Crop ID (label).
                The data points with the same Crop ID (label) are the same color. The location on the y-axis where a data
                point's line crosses corresponds to its value for that given feature. This is where the tool becomes extremely
                powerful.<br><br>
                The tool allows us to create bypass filters to look at specific ranges for each feature or label. Every data
                point satisfying the filter range will be highlighted and the rest will be muted. For example, to look at
                data points with only Crop ID 4, click and drag directly on the Crop ID y-axis so that a filter is created
                over the value 4.<br><br>
                As you will see, only the lines corresponding to Crop ID 4 will be shown. This filter can be moved by
                dragging it up or down. You can very easily chain filters by clicking and dragging on different feature axes,
                creating a set of feature ranges. You can also add more than one filter per axis!<br><br>
                To remove all filters on an axis, just click the axis once, without dragging. That's it! See if you can create
                a filter set that shows only a single Crop ID. This is exactly what our Machine Learning models are trying to do in
                different ways!
              </div>
            </div>
          </div>
        </p>
      </div>
      <div style="text-align:center;">
        <button onclick="applyFilters(1)">Apply Example Filter</button>
        <button onclick="applyFilters(0)">Remove Example Filter</button>
      </div>
       <div style="max-width: 1200px; margin: auto;" id="feature-analysis-plot"></div>
    </section>

    <section>
    <div class="main-content">
      <h3>Training and Testing Dataset:
        <a id="Training-testing-dataset" class="anchor" href="#creating-pages-manually" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span>
        </a></h3>
        <p>
          For our training and testing dataset, we used the images provided with a known Field ID. We randomly chose
          80% of stratified image samples to train our model, leaving 20% left to test the model. This comes out to
          roughly 38,000  data points to train <span lang="latex">(N_{train})</span> and 9,500 data points to test
          <span lang="latex">(N_{test})</span>. All of the images
          and label arrays are sized to be 2016 × 3035 pixels. The amount of data points within each image
          varies depending on satellite image. View <a href="#image-visualizer">here</a> to see how each image is broken
          down into its features and components.<br><br>
          For our Machine Learning Models, the feature data <span lang="latex">(X)</span> is most easily broken down into a 3-dimnesional array.
          Because the data is a time-series, there are repetitive data points corresponding to the same field location
          in the image. The size of the input and label arrays <span lang="latex">(X, y)</span> are<br><br>
          <div style="margin:auto; max-width: 330px;">
            <div style="text-align:center;">
              <span lang="latex"> X \in \mathbb{R}^{N \times B \times D}</span><br>
              <span lang="latex"> y \in \mathbb{R}^{N}</span><br><br>
            </div>
            <span lang="latex">N = \text{Number of Data Points}</span><br>
            <span lang="latex">B = \text{Number of Spectral Bands}</span><br>
            <span lang="latex">D = \text{Number of Dates in the Time Series}</span><br><br>
          </div>
          Most Machine Learning Models cannot ingest higher dimensional arrays, so the first step before inputting the data
          is to flatten in. The process of flattening takes each multi-dimensional index and moves it into a single dimensional array.
          This mapping from 3-dimensions to 1-dimension looks like this mathematically,<br><br>
          <div style="text-align:center;">
            <span lang="latex">f: (n, b, d) \rightarrow (i) = n \cdot b \cdot d</span><br><br>
          </div>
          The flattened training and testing feature arrays with then have the following shape,<br><br>
          <div style="text-align:center;">
            <span lang="latex">X_{train} \subset \mathbb{R}^{\sim 6422000}</span><br>
            <span lang="latex">X_{test} \subset \mathbb{R}^{\sim 1605500}</span><br><br>
          </div>
          The labels for classification <span lang="latex">(y)</span>, is simply a column vector of length
          <span lang="latext">N</span>.<br><br>
          <div style="text-align:center;">
            <span lang="latex">y_{train} \subset \mathbb{R}^{\sim 38000}</span><br>
            <span lang="latex">y_{test} \subset \mathbb{R}^{\sim 9500}</span><br><br>
          </div>
        </p>
    </div>
    </section>
     <section>
      <div class="main-content">

      <h2>
        Strategy
        <a id="Strategy" class="anchor" href="#creating-pages-manually" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span>
        </a>
      </h2>
      <p>
        We decided to approach this problem using two different methods:
    <ol>
        <li>Artificial Neural Networks (ANN) Multi-Layered Perceptron (MLP) Model</li>
        <li>Multi-Class Support Vector Machines (MSVM) Model</li>
    </ol>
      </p>
    <h4>Hypotheses and Pre-Processing</h4>
        <p>
          <ol>
        <li><span style="font-weight: bold;">If only the visible spectrum bands are used, then the model will improve in accuracy.</span><br>
          <p>Due to the biology of plant species, studies have show that plants emit and absorb the most light in the
          visible spectrum. Therefore, according to information theory, there is more entropy in those spectra. As a general rule of thumb,
          it is important to have many more data points than features when training an ANN.
          </p>
        </li>
        <li><span style="font-weight: bold;">If the epochs used in the ANN are increased to above 500, then the model will begin to decrease in accuracy.</span><br>
          <p>A common phenomenon seen in ANN models is over-fitting. If too many epochs are run, the model begins to memorize
            the training data. This causes the model to fail when predicting on unseen testing data. We expect this to be around
            500 epochs.
          </p>
        </li>
         <li><span style="font-weight: bold;">If both an ANN and a MSVM are implemented, then the ANN will outperform the MSVM with respect to accuracy.</span><br>
        <p>
            When the two approaches are compared, the Neural Network approach will beat out the Support Vector Machines.
            We think this because Neural networks are turing complete, that is they can be fit to represent any
            programmable function, whereas support vector machines have a performance cap dependent on their
            hyper-parameters.
        </p>
      </li>
        </ol>
        </p>
    </div>
    </section>


    <section>
      <div class="main-content">
        <h2>
          Artificial Neural Network (ANN):<br> Multi-Layered Perceptron Model
          <a id="ANN" class="anchor" href="#creating-pages-manually" aria-hidden="true">
            <span aria-hidden="true" class="octicon octicon-link"></span>
          </a>
        </h2>
        <p>
        A Multi-Layered Perceptron (MLP), is a supervised learning algorithm that consists of an input, several hidden
          layers, and an output later. Each hidden layer has nodes that use activation functions. MLP then uses
          backpropogation to train the network and update the nodes. We chose to use this approach because the MLP
          can capture more complex and non-linear features in the dataset. Our MLP Model will use the twelve spectrum
          imaging bands and cloud data per field id in a single aggregated matrix form. Each matrix entry represents a
          pixel corresponding to a geographical field location. The output will be a column vector containing the
          probability that crop X was in that field id.
        </p>
        <p>
          We performed two different studies using the ANN model. The first was an activation study which tested how
          the loss and accuracy changed from using different activation functions for different epoch values. We also
          performed a hidden layer study in which we varied the number of hidden layers used. For both of these studies
          we utilized the functions built in to the Keras API.
        </p>

        <h4>Activation Function Study:</h4>
        <p> For this study we varied activation functions for different epoch values. One epoch is when the entire
          dataset is passed forward and backward through the neural network. As the number of epochs are increased
          the weights are changed in the neural network nore and the model will change from underfitting the data
          to becoming optimal and then overfitting which is why experimentation is needed for different datasets to
          determine the optimal epoch value. An activation function needs to be applied for a MLP model to allow the
          network to represent non-linear complex functional mappings between inputs and outputs. The optimal activaton
          function varies between datasets which is why we also varied the activation functions used for this study.
          We recorded losses and accuracy to determine the optimal epoch value and activation function for our dataset.
        </p>

        <div style="max-width: 1200px; margin: auto;" id="activation-function-study-softplus"></div>
        <div style="max-width: 1200px; margin: auto;" id="activation-function-study-uniform"></div>

        <h4>Hidden Layer Study:</h4>
        <p>For our hidden layer study we also varied the number of epochs for the reason explained in the previous
          section. For this study we also varied the number of hidden layers. A single-layer artificial neural
          network has a single layer of nodes and each node connects directly to an input variable and then contributes
          to an output variable. To create a multilayer perceptron a single layer network is extended where it has an
          input layer that connects to the input variables, one or more hidden layers, and an output layer that produces
          the output variables. Because of the complexity of our dataset, a multi-layered model can be more accurate.
          To determine the optimal number of layers for our model, we need to experiment with different numbers of them
          which is what we do in this study.
        </p>

        <div style="max-width: 1200px; margin: auto;" id="hidden-layer-study"></div>

        <h4>Batch Size Study:</h4>
        <p>Using an epoch of 250, we vary the batch size used to train our neural network using our. As expected, we
            find that higher batch sizes leads to more inaccurate data than lower batch sizes. However, lowering the
            batch size does not always decrease accuracy as we can see that accuracy begins to decrease when our batch
            size is smaller than 32. From our study, the optimal batch size is 32. This study is important because it
            allows us to find the optimal batch size that is a trade off between accuracy and execution time.
        </p>
        <div style="max-width: 1200px; margin: auto;" id="Batch_size"></div>

        <h4>PCA Study:</h4>
        <p>We tried to reduce the features of our datasets in order to improve accuracy and performance. Because each
            pixel had twelve bands (not including CLD) and 13 time-series, the size of our feature was 156. We try to
            further reduce with PCA, varying the number of PCA dimensions from 1 to 8. Running our model with an epoch
            of 250, we find no significant changes in accuracy between the PCA datasets. We also do not see an execution
            time change in PCA data. Therefore, we do not consider PCA impactful to our study. However, it might preserve
            memory usage in our model.
        </p>
        <div style="max-width: 1200px; margin: auto;" id="PCA_dims"></div>


        <h4>ANN Model Results:</h4>
        <div style="max-width: 1200px; margin: auto;" id="epochs"></div>
        <p>We borrow ideas from models commonly trained with the Fashion MNIST dataset.
            We have an input layer that is flattened for every pixel (156 in size), and we utilize two hidden layers
            activated with ReLu, connected to an output layer of size 7 for classification. While testing our
            hyperparameters, we observe that the data in the cloud probability is vastly different from the data from
            the other bands. Removing the cloud probability layer completely produced higher accuracy results, so we
            removed it from our model.
        </p>
      </div>
    </section>


     <section>
      <div class="main-content">
      <h2>
        Multi-Class Support Vector Machines (MSVM) Model

        <a id="MSVM" class="anchor" href="#creating-pages-manually" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span>
        </a>
      </h2>
      <p>
        We chose to test a multi-class support vector machine model because SVMs are often used for classification
        tasks and tend to perform well with many problem types. An SVM constructs hyperplanes in multidimensional
        space that separates cases of different class labels. If your dataset has more than two classes (non-binary
        classification) then you use a multi-class suppor vector machine model. An SVM classifies data by determining
        the optimal hyperplane that separates observations according to their class labels. The main idea when using a
        MSVM is to transform the data to a higher dimension so hopefully the data will be linearly separable. Data
        points of one class can be mapped using mathematical functions called kernels and the mapped data points allow
        the MSVM model to find the optimal line to separate the classes. Different kernel types can be used and gamma
        and cost arguments can be varied where gamma represents the argument of the kernel function and cost specifies
        the cost of a violation to the margin. Experimenting with these values helps to find the best classification
        accuracy.
      </p>
      <h4>Methodology</h4>
      <p>
        In our case, we used Scikit learning’s implementation of the MSVC, which provides 3 options for built in Kernels (Linear, Polynomial and Radial Basis Function). We went with Radial Basis Functions, which has two parameters, gamma and C, that influence the shape of the decision boundary. Gamma describes how ‘curvy’ our decision boundary will be. The lower the boundary, the closer it is to a straight line. The higher, the more the distance to a labelled point determines the decision boundary. C describes the softness of decision boundary. Closer to zero indicates a decision boundary more tolerant of error, and the greater C the less tolerant or error the boundary becomes. From here, we create a small selection of possible gamma and C values and used a grid search algorithm to find the best possible hyperparameters.
      </p>
      <p>
        Using this methodology, we began to test training our model with the entirety of our dataset. Our goal at the outset was to reach an accuracy of 80%. Our immediate realization is that the time cost was so large, that we wanted to test to see how taking smaller samples of the data would impact our result. We decided to take records of accuracy and time spent on these smaller samples and record their performance.
      </p>
      <p>
        For our experiment, we took 30 samples of random partitions of the data such that each data point had a Possibility ‘P’ of being included in the training. So if P = 0.1, on average ten percent of training data was fit before testing.
        In addition, we came to this with some background understanding that plant chlorophyll drops significantly outside of a sweet spot within the visible light spectrum. Thus, we believed that the removal of data representing electromagnetic wavelengths outside the spectrum would not impact accuracy.
      </p>
      <h4>MSVM Study Results</h4>
       <div style="max-width: 1200px; margin: auto;" id="RGB-Band-Studies"></div>
      <div style="max-width: 1200px; margin: auto;" id="RGB-Band-Studies-Time"></div>
      <p>
        As you can see in the following graphs, the accuracy increases consistently as the amount of training data used approaches 100%. Accuracy at 0.9 exceeds 80%, our threshold for satisfaction. We can also see that this threshold is reached by smaller subsets of the data. Most interestingly, we see that we can arrive at the 80% accuracy using only the bands of electromagnetic spectrum within the visible light spectrum, and 80% of our training data. The time taken to get to this result was approximately 35% of the cost of our most expensive observation, 90% of training data across all bands.
      </p>
      <p>
        The loss of accuracy when dropping all electromagnetic bands outside of the visible spectrum never exceeded 5
        percent, but more than halved the time taken to train the SVM model (except in the smallest proportion of data
        10%) This has important applications, because it reveals a way we can optimize our collection of data in the
        future. For example, if our analysis is time sensitive, we can reduce our training time ~50% by reducing features
        to RGB, and still expect results of at least 80% accuracy.
      </p>
      <!--<h4>Confusion Matrix </h4>-->
      <!--<p>-->
        <!--... fill in later-->
      <!--</p>-->
      <!--<div id="confusion-matrix"></div>-->
      </div>
    </section>

      <section>
      <div class="main-content">
      <h2>
        Overall Results
        <a id="Results" class="anchor" href="#creating-pages-manually" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span>
        </a>
      </h2>
       <div style="max-width: 1200px; margin: auto;" id="All-Studies-Accuracy"></div>
      <p>
        When comparing the MSVM Model to the Neural Networks from earlier, we see that accuracy is comparable between the two methods, with a slight edge for the Neural Network in the end. The Neural Network predictions were 92% for the final model, compared to 86% in the all data case for MCSVM. This shows that Neural Networks perform slightly better than our implementation for multi-classification support vector machines.
      </p>
      <span style="text-align: center;">
          <h5>Best ANN Accuracy: 92%</h5>
          <h5>Best MSVM Accuracy: 86%</h5>
      </span>

      </div>
    </section>

     <section>
      <div class="main-content">
      <h2>Discussion
        <a id="Discussion" class="anchor" href="#creating-pages-manually" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span>
        </a>
      </h2>
      <p>
        The overall implementation of a successful Machine Learning Model required extensive research, testing,
          and model modification. The take home message is that there is no "one-size-fits-all" model that will produce
          highly accurate results. Furthermore, the majority of the hypotheses we made were not supported, even though
          most of them were grounded in scientific fact. The best approach we used was to vary only one hyperparameter at a time for
          each model to see its impact on accuracy. We then combine the best performing hyperparameters to achieve a well-rounded
          high performing model.<br>
      </p>
      <h4>Hypothesis Discussion</h4>
      <p>
        <ol>
      <li><span style="font-weight: bold;">If only the visible spectrum bands are used, then the model will improve in accuracy.</span><br>
        <p>The data suggests that this hypothesis <span style="font-weight: bold;">was not supported</span>. We see better model performance for both
        the ANN and the MSVM when we include all avaiable frequency spectra. This is curious, because we were expecting the
        model to become "confused" or over-fitted with too many features. But we see that we have enough data points to have 13 features.
      </p>
      </li>
      <li><span style="font-weight: bold;">If the epochs used in the ANN are increased to above 500, then the model will begin to decrease in accuracy.</span><br>
        <p>The data suggests that this hypothesis <span style="font-weight: bold;">was not supported</span>. We ran up to 1000 epochs for the ANN model
          and found that the accuracy continues to increase past 500 epochs. This is a little surprising, as we expected to see the
          model over-fit the training data. What this means is that we have enough data to run many more epochs. We would need to run
          potentially thousands more epochs in order to see the over-fitting phenomenon.
        </p>
      </li>
       <li><span style="font-weight: bold;">If both an ANN and a MSVM are implemented, then the ANN will outperform the MSVM with respect to accuracy.</span><br>
        <p>
            The data suggests that this hypothesis <span style="font-weight: bold;">was supported</span>.During the development of our models, we initially saw comparable scores coming out of our Neural Network
            and our Support Vector machine, when we were doing a grid search over possible gamma and C values for our
            SVM. As the Neural Network schema became more complex, it eventually achieved up to a 92% accuracy,
            whereas the SVM never exceeded 90%.
        </p>
      </li>
      </ol>
      </p>
    </div>
    </section>

     <section>
      <div class="main-content">
      <h2>
        Authors and Contributors
        <a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span>
        </a>
      </h2>
      <p>
        Luka Derado, Benjamin Daghir, Erin McCaskey, Khang Vu
      </p>
    </div>
    </section>

     <section>
      <div class="main-content">
      <h3>
        Support or Contact
        <a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true">
          <span aria-hidden="true" class="octicon octicon-link"></span>
        </a>
      </h3>
      <p>
        Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a>
        or <a href="https://github.com/contact">contact support</a> and weâ€™ll help you sort it out.
      </p>
    </div>
    </section>

    <section class="main-content">
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.gatech.edu/kvu35/CS4641-Project">
          Cs4641-project</a> is maintained by <a href="https://github.gatech.edu/kvu35">kvu35</a>.</span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">
          GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a>
          by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>
    </section>
  </body>

  <script src="js/main.js"></script>
  <script src="js/plotly.js"></script>
</html>
